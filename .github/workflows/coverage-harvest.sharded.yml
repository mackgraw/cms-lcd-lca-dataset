name: Coverage Harvest (sharded, artifacts-only)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 6 * * *" # optional daily run

concurrency:
  group: coverage-harvest-${{ github.ref }}
  cancel-in-progress: true

jobs:
  harvest:
    name: harvest shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    env:
      PYTHONPATH: ${{ github.workspace }}
    strategy:
      fail-fast: false
      max-parallel: 4
      matrix: { shard: [0, 1, 2, 3] }  # adjust shard count as needed
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install -U pip
          [[ -f requirements.txt ]] && pip install -r requirements.txt || echo "no requirements.txt"

      - name: Run shard (smart entrypoint)
        env:
          SHARD_INDEX: ${{ matrix.shard }}
          SHARD_COUNT: 4         # keep for legacy; entrypoint maps this…
          SHARD_TOTAL: 4         # …but set SHARD_TOTAL explicitly for your harvester
          COVERAGE_MAX_DOCS: ${{ vars.COVERAGE_MAX_DOCS }}
          COVERAGE_STATUS: ${{ vars.COVERAGE_STATUS }}
          COVERAGE_STATES: ${{ vars.COVERAGE_STATES }}
          COVERAGE_CONTRACTORS: ${{ vars.COVERAGE_CONTRACTORS }}
          COVERAGE_TIMEOUT: 60
        run: |
          bash scripts/harvest_entrypoint.sh shard_out
          ls -l shard_out || true

      - name: Upload shard artifact
        uses: actions/upload-artifact@v4
        with:
          name: shard_${{ matrix.shard }}
          path: shard_out/
          retention-days: 7

  merge_and_package:
    name: merge and package
    needs: [harvest]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Download all shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: shards_in

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Combine shard CSVs -> dataset/*.csv
        run: |
          mkdir -p dataset
          python scripts/combine_sharded_outputs.py --in shards_in --out dataset

      - name: List dataset
        run: ls -lah dataset || true

      - name: Upload merged dataset as artifact (for official release workflow)
        uses: actions/upload-artifact@v4
        with:
          name: dataset_artifact
          path: dataset/*.csv
          retention-days: 14

      - name: (Optional) Upload zipped dataset for inspection
        if: ${{ hashFiles('dataset/*.csv') != '' }}
        run: zip -r dataset.zip dataset

      - name: Upload zipped dataset (inspection only)
        if: ${{ hashFiles('dataset/*.csv') != '' }}
        uses: actions/upload-artifact@v4
        with:
          name: dataset_zip
          path: dataset.zip
          retention-days: 7
