name: Coverage Harvest (Sharded, Daily 6am ET)

on:
  # Run at 10:00 and 11:00 UTC; ET guard below will only proceed when it's 06:00 in America/New_York
  schedule:
    - cron: "0 10 * * *"
    - cron: "0 11 * * *"
  workflow_dispatch:
    inputs:
      shards:
        description: "Number of shards (>=1)"
        default: "4"
        required: false
      max_docs:
        description: "Optional cap on documents BEFORE sharding (blank = all)"
        default: ""
        required: false
      timeout:
        description: "HTTP timeout (seconds)"
        default: "30"
        required: false

env:
  PYTHONDONTWRITEBYTECODE: "1"
  PIP_DISABLE_PIP_VERSION_CHECK: "1"

jobs:
  # Gate so we truly run at 06:00 ET regardless of DST
  time_gate:
    runs-on: ubuntu-latest
    outputs:
      run_now: ${{ steps.gate.outputs.run_now }}
    steps:
      - id: gate
        name: Only proceed when local ET hour == 06
        shell: bash
        run: |
          set -euo pipefail
          tz="America/New_York"
          now_et_hour="$(TZ=$tz date +'%H')"
          if [ "$now_et_hour" = "06" ]; then
            echo "run_now=true" >> "$GITHUB_OUTPUT"
            echo "Proceeding: Local ET hour is 06."
          else
            echo "run_now=false" >> "$GITHUB_OUTPUT"
            echo "Skipping: Local ET hour is $now_et_hour (not 06)."
          fi

  # Build the shard matrix from the workflow_dispatch input (or default 4)
  compute_matrix:
    needs: [time_gate]
    if: needs.time_gate.outputs.run_now == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    outputs:
      shards_json: ${{ steps.mk.outputs.shards_json }}
      shard_total: ${{ steps.mk.outputs.shard_total }}
      max_docs: ${{ steps.mk.outputs.max_docs }}
      timeout: ${{ steps.mk.outputs.timeout }}
    steps:
      - id: mk
        name: Produce shard matrix JSON
        shell: bash
        run: |
          set -euo pipefail
          n="${{ github.event.inputs.shards || '4' }}"
          [[ "$n" =~ ^[0-9]+$ ]] && [ "$n" -ge 1 ] || n=4
          arr="["
          for ((i=0;i<n;i++)); do
            if [ $i -gt 0 ]; then arr="$arr, "; fi
            arr="$arr$i"
          done
          arr="$arr]"
          echo "shards_json=$arr"       >> "$GITHUB_OUTPUT"
          echo "shard_total=$n"         >> "$GITHUB_OUTPUT"
          echo "max_docs=${{ github.event.inputs.max_docs || '' }}" >> "$GITHUB_OUTPUT"
          echo "timeout=${{ github.event.inputs.timeout  || '30' }}" >> "$GITHUB_OUTPUT"
          echo "Matrix: $arr (total=$n)"

  harvest:
    needs: [compute_matrix]
    if: always() && (needs.compute_matrix.result == 'success')
    runs-on: ubuntu-latest
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        shard_index: ${{ fromJSON(needs.compute_matrix.outputs.shards_json) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt

      - name: Run shard
        env:
          COVERAGE_TIMEOUT: ${{ needs.compute_matrix.outputs.timeout }}
          COVERAGE_MAX_DOCS: ${{ needs.compute_matrix.outputs.max_docs }}
          SHARD_INDEX: ${{ matrix.shard_index }}
          SHARD_TOTAL: ${{ needs.compute_matrix.outputs.shard_total }}
        run: |
          set -euo pipefail
          echo "[env] SHARD_INDEX=${SHARD_INDEX} SHARD_TOTAL=${SHARD_TOTAL}"
          python -m scripts.harvest_shard

      - name: Collect shard outputs
        run: |
          set -euo pipefail
          mkdir -p shard_out
          # Each shard writes to dataset/document_*_shard_{i}_of_{n}.csv
          cp -v dataset/document_*_shard_*_of_* . || true
          mv -v document_*_shard_*_of_* shard_out/ || true
          echo "[ls] shard_out"; ls -l shard_out || true

      - name: Upload shard artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dataset-shards
          path: shard_out/*

  merge_and_release:
    needs: [harvest, compute_matrix, time_gate]
    if: needs.time_gate.outputs.run_now == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    permissions:
      contents: write   # REQUIRED to create releases
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps + gh CLI
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install -U pip
          pip install -r requirements.txt
          sudo apt-get update -y
          sudo apt-get install -y gh

      - name: Download shard artifacts
        uses: actions/download-artifact@v4
        with:
          name: dataset-shards
          path: artifacts

      - name: Merge shards -> dataset/document_*.csv
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p dataset

          merge_one() {
            pattern="$1"
            out="$2"
            : > "$out"
            first=1
            while IFS= read -r -d '' f; do
              if [ $first -eq 1 ]; then
                cat "$f" >> "$out"
                first=0
              else
                tail -n +2 "$f" >> "$out"
              fi
            done < <(find artifacts -type f -name "$pattern" -print0 | sort -z)
          }

          merge_one "document_codes_shard_*_of_*.csv"   "dataset/document_codes.csv"
          merge_one "document_nocodes_shard_*_of_*.csv" "dataset/document_nocodes.csv"

          echo "[ls] artifacts"; ls -R artifacts || true
          echo "[ls] dataset after merge"; ls -l dataset

      - name: Fetch previous normalized (if exists)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p prev
          if gh release view --repo "$GITHUB_REPOSITORY" >/dev/null 2>&1; then
            gh release download --repo "$GITHUB_REPOSITORY" --pattern "codes_normalized.csv" --dir prev || true
          fi
          # Create header-only baseline if nothing fetched
          if [ ! -s prev/codes_normalized.csv ]; then
            echo "doc_type,doc_id,code_system,code,description,coverage_flag" > prev/codes_normalized.csv
          fi
          echo "[ls] prev"; ls -l prev

      - name: Compute normalized + changes
        shell: bash
        run: |
          set -euo pipefail
          # Writes dataset/codes_normalized.csv and dataset/codes_changes.csv
          python -m scripts.compute_changes dataset/document_codes.csv prev/codes_normalized.csv
          echo "[ls] dataset after compute_changes"; ls -l dataset

      - name: Sanity check expected files
        shell: bash
        run: |
          set -euo pipefail
          test -s dataset/document_codes.csv
          test -s dataset/document_nocodes.csv
          test -s dataset/codes_normalized.csv
          test -s dataset/codes_changes.csv
          echo "[ok] All required files present."

      - name: Create dated ZIP
        id: mkzip
        shell: bash
        run: |
          set -euo pipefail
          ts="$(date -u +%Y%m%d-%H%M%S)"
          zipname="dataset-${ts}.zip"
          (cd dataset && zip -r "../${zipname}" ./*.csv)
          echo "zipname=${zipname}" >> "$GITHUB_OUTPUT"
          echo "[ls] root after zip"; ls -l

      - name: Create GitHub Release (latest)
        uses: softprops/action-gh-release@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          tag_name: v${{ github.run_id }}-${{ github.run_attempt }}
          name: "Daily dataset ${{ github.run_id }}-${{ github.run_attempt }}"
          generate_release_notes: true
          make_latest: true
          files: |
            dataset/document_codes.csv
            dataset/document_nocodes.csv
            dataset/codes_normalized.csv
            dataset/codes_changes.csv
            ${{ steps.mkzip.outputs.zipname }}
