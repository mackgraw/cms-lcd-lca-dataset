name: Coverage Harvest (Sharded, Daily 6am ET)

on:
  # Run at 10:00 and 11:00 UTC; guard below only proceeds when it's 06:00 in America/New_York
  schedule:
    - cron: "0 10 * * *"
    - cron: "0 11 * * *"
  workflow_dispatch:
    inputs:
      shards:
        description: "Number of shards (>=1)"
        default: "4"
        required: false
      max_docs:
        description: "Optional cap on documents BEFORE sharding (blank = all)"
        default: ""
        required: false
      timeout:
        description: "HTTP timeout (seconds)"
        default: "30"
        required: false

env:
  PYTHONDONTWRITEBYTECODE: "1"
  PIP_DISABLE_PIP_VERSION_CHECK: "1"

permissions:
  contents: read

jobs:
  time_gate:
    runs-on: ubuntu-latest
    outputs:
      run_now: ${{ steps.gate.outputs.run_now }}
    steps:
      - id: gate
        name: Only proceed when local ET hour == 06
        shell: bash
        run: |
          set -euo pipefail
          tz="America/New_York"
          now_et_hour="$(TZ=$tz date +'%H')"
          if [ "$now_et_hour" = "06" ]; then
            echo "run_now=true" >> "$GITHUB_OUTPUT"
            echo "Proceeding: Local ET hour is 06."
          else
            echo "run_now=false" >> "$GITHUB_OUTPUT"
            echo "Skipping: Local ET hour is $now_et_hour (not 06)."
          fi

  compute_matrix:
    needs: [time_gate]
    if: needs.time_gate.outputs.run_now == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    outputs:
      shards_json: ${{ steps.mk.outputs.shards_json }}
      shard_total: ${{ steps.mk.outputs.shard_total }}
      max_docs: ${{ steps.mk.outputs.max_docs }}
      timeout: ${{ steps.mk.outputs.timeout }}
    steps:
      - id: mk
        name: Produce shard matrix JSON
        shell: bash
        run: |
          set -euo pipefail
          n="${{ github.event.inputs.shards || '4' }}"
          [[ "$n" =~ ^[0-9]+$ ]] && [ "$n" -ge 1 ] || n=4
          arr="["
          for ((i=0;i<n;i++)); do
            if [ $i -gt 0 ]; then arr="$arr, "; fi
            arr="$arr$i"
          done
          arr="$arr]"
          echo "shards_json=$arr"       >> "$GITHUB_OUTPUT"
          echo "shard_total=$n"         >> "$GITHUB_OUTPUT"
          echo "max_docs=${{ github.event.inputs.max_docs || '' }}" >> "$GITHUB_OUTPUT"
          echo "timeout=${{ github.event.inputs.timeout  || '30' }}" >> "$GITHUB_OUTPUT"
          echo "Matrix: $arr (total=$n)"

  harvest:
    needs: [compute_matrix]
    if: always() && (needs.compute_matrix.result == 'success')
    runs-on: ubuntu-latest
    permissions:
      contents: read
    strategy:
      fail-fast: false
      matrix:
        shard_index: ${{ fromJSON(needs.compute_matrix.outputs.shards_json) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt

      - name: Run shard
        env:
          COVERAGE_TIMEOUT: ${{ needs.compute_matrix.outputs.timeout }}
          COVERAGE_MAX_DOCS: ${{ needs.compute_matrix.outputs.max_docs }}
          SHARD_INDEX: ${{ matrix.shard_index }}
          SHARD_TOTAL: ${{ needs.compute_matrix.outputs.shard_total }}
        run: |
          set -euo pipefail
          echo "[env] SHARD_INDEX=${SHARD_INDEX} SHARD_TOTAL=${SHARD_TOTAL}"
          python -m scripts.harvest_shard
          echo "[ls] dataset after shard ${SHARD_INDEX}"
          ls -l dataset || true

      - name: Verify shard produced outputs
        shell: bash
        run: |
          set -euo pipefail
          codes="dataset/document_codes_shard_${{ matrix.shard_index }}_of_${{ needs.compute_matrix.outputs.shard_total }}.csv"
          nocodes="dataset/document_nocodes_shard_${{ matrix.shard_index }}_of_${{ needs.compute_matrix.outputs.shard_total }}.csv"
          missing=0
          for f in "$codes" "$nocodes"; do
            if [ ! -s "$f" ]; then
              echo "::error file=$f::Expected shard output not found for shard ${{ matrix.shard_index }} of ${{ needs.compute_matrix.outputs.shard_total }}"
              missing=1
            fi
          done
          if [ "$missing" -ne 0 ]; then
            echo "Shard ${{ matrix.shard_index }}: required output files are missing"; exit 1
          fi
          echo "[ok] shard ${{ matrix.shard_index }} produced:"
          ls -l "$codes" "$nocodes"

      - name: Upload shard artifacts
        uses: actions/upload-artifact@v4
        with:
          name: shard-${{ matrix.shard_index }}
          path: |
            dataset/document_codes_shard_${{ matrix.shard_index }}_of_${{ needs.compute_matrix.outputs.shard_total }}.csv
            dataset/document_nocodes_shard_${{ matrix.shard_index }}_of_${{ needs.compute_matrix.outputs.shard_total }}.csv
          if-no-files-found: error

  merge_and_release:
    needs: [harvest, compute_matrix, time_gate]
    if: needs.time_gate.outputs.run_now == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    permissions:
      contents: write   # REQUIRED to create releases
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps + gh CLI
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install -U pip
          pip install -r requirements.txt
          sudo apt-get update -y
          sudo apt-get install -y gh

      - name: Download ALL shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge shards -> dataset/document_*.csv
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p dataset

          merge_one() {
            pattern="$1"
            out="$2"
            : > "$out"
            first=1
            while IFS= read -r -d '' f; do
              if [ $first -eq 1 ]; then
                cat "$f" >> "$out"
                first=0
              else
                tail -n +2 "$f" >> "$out"
              fi
            done < <(find artifacts -type f -name "$pattern" -print0 | sort -z)
          }

          merge_one "document_codes_shard_*_of_*.csv"   "dataset/document_codes.csv"
          merge_one "document_nocodes_shard_*_of_*.csv" "dataset/document_nocodes.csv"

          echo "[ls] artifacts"; ls -R artifacts || true
          echo "[ls] dataset after merge"; ls -l dataset

      - name: Fetch previous normalized (if exists)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p prev
          if gh release view --repo "$GITHUB_REPOSITORY" >/dev/null 2>&1; then
            gh release download --repo "$GITHUB_REPOSITORY" --pattern "codes_normalized.csv" --dir prev || true
          fi
          # Create header-only baseline if nothing fetched
          if [ ! -s prev/codes_normalized.csv ]; then
            echo "doc_type,doc_id,code_system,code,description,coverage_flag" > prev/codes_normalized.csv
          fi
          echo "[ls] prev"; ls -l prev

      - name: Build codes_normalized.csv (inline, Article+LCD)
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
import csv, sys
inp = "dataset/document_codes.csv"
outp = "dataset/codes_normalized.csv"
cols_out = ["doc_type","doc_id","code_system","code","description","coverage_flag"]
with open(inp, newline="", encoding="utf-8") as fin, open(outp, "w", newline="", encoding="utf-8") as fout:
    r = csv.DictReader(fin)
    w = csv.DictWriter(fout, fieldnames=cols_out); w.writeheader()
    for row in r:
        doc_type = (row.get("document_type") or row.get("doc_type") or "").strip()
        doc_id = (row.get("document_id") or row.get("doc_id") or row.get("article_id") or "").strip()
        code_system = (row.get("code_system") or "").strip()
        code = (row.get("code") or "").strip()
        description = (row.get("description") or "").strip()
        coverage_flag = (row.get("coverage_flag") or "").strip()
        if not doc_id or not code:  # skip blanks
            continue
        w.writerow({
            "doc_type": doc_type if doc_type in ("LCD","Article") else doc_type,
            "doc_id": doc_id,
            "code_system": code_system,
            "code": code,
            "description": description,
            "coverage_flag": coverage_flag,
        })
print(f"[ok] wrote dataset/codes_normalized.csv")
PY
          echo "[ls] dataset after normalization"; ls -l dataset

      - name: Build codes_changes.csv (inline diff vs prev)
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
import csv, sys
from pathlib import Path

def read_rows(p):
    rows=[]
    with open(p, newline="", encoding="utf-8") as f:
        r=csv.DictReader(f)
        for row in r: rows.append(row)
    return rows

prev_p = Path("prev/codes_normalized.csv")
curr_p = Path("dataset/codes_normalized.csv")

prev = {(r["doc_type"], r["doc_id"], r["code_system"], r["code"]): (r.get("coverage_flag","") or "") for r in read_rows(prev_p)}
curr = {(r["doc_type"], r["doc_id"], r["code_system"], r["code"]): (r.get("coverage_flag","") or "") for r in read_rows(curr_p)}

keys = set(prev.keys()) | set(curr.keys())
out_cols = ["change_type","doc_type","doc_id","code_system","code","prev_flag","curr_flag"]
with open("dataset/codes_changes.csv","w",newline="",encoding="utf-8") as f:
    w=csv.DictWriter(f, fieldnames=out_cols); w.writeheader()
    for k in sorted(keys):
        p = prev.get(k); c = curr.get(k)
        if p is None and c is not None:
            ct="Added"
        elif p is not None and c is None:
            ct="Removed"
        elif p != c:
            ct="FlagChanged"
        else:
            continue
        dt, di, cs, code = k
        w.writerow({"change_type":ct,"doc_type":dt,"doc_id":di,"code_system":cs,"code":code,"prev_flag":("" if p is None else p),"curr_flag":("" if c is None else c)})
print("[ok] wrote dataset/codes_changes.csv")
PY
          echo "[ls] dataset after compute_changes"; ls -l dataset

      - name: Sanity check expected files
        shell: bash
        run: |
          set -euo pipefail
          test -s dataset/document_codes.csv
          test -s dataset/document_nocodes.csv
          test -s dataset/codes_normalized.csv
          test -s dataset/codes_changes.csv
          echo "[ok] All required files present."

      - name: Create dated ZIP
        id: mkzip
        shell: bash
        run: |
          set -euo pipefail
          ts="$(date -u +%Y%m%d-%H%M%S)"
          zipname="dataset-${ts}.zip"
          (cd dataset && zip -r "../${zipname}" ./*.csv)
          echo "zipname=${zipname}" >> "$GITHUB_OUTPUT"
          echo "[ls] root after zip"; ls -l

      - name: Create GitHub Release (latest)
        uses: softprops/action-gh-release@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          tag_name: v${{ github.run_id }}-${{ github.run_attempt }}
          name: "Daily dataset ${{ github.run_id }}-${{ github.run_attempt }}"
          generate_release_notes: true
          make_latest: true
          files: |
            dataset/document_codes.csv
            dataset/document_nocodes.csv
            dataset/codes_normalized.csv
            dataset/codes_changes.csv
            ${{ steps.mkzip.outputs.zipname }}
