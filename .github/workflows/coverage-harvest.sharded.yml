name: coverage-harvest-sharded

on:
  workflow_dispatch:
    inputs:
      shards:
        description: "Number of shards (>=1)"
        default: "4"
      max_docs:
        description: "Optional limit for reports list (applied before sharding)"
        default: ""
      timeout:
        description: "HTTP timeout (seconds)"
        default: "30"
  schedule:
    - cron: "0 6 * * *"  # daily at 06:00 UTC

jobs:
  compute-matrix:
    runs-on: ubuntu-latest
    outputs:
      shards: ${{ steps.mk.outputs.shards }}
      total: ${{ steps.mk.outputs.total }}
      max_docs: ${{ steps.mk.outputs.max_docs }}
      timeout: ${{ steps.mk.outputs.timeout }}
    steps:
      - id: mk
        shell: bash
        run: |
          n="${{ github.event.inputs.shards || '4' }}"
          [[ "$n" =~ ^[0-9]+$ ]] && [ "$n" -ge 1 ] || n=4
          arr="["
          for ((i=0; i<n; i++)); do
            if [ $i -gt 0 ]; then arr="$arr, "; fi
            arr="$arr$i"
          done
          arr="$arr]"
          echo "shards=$arr" >> "$GITHUB_OUTPUT"
          echo "total=$n" >> "$GITHUB_OUTPUT"
          echo "max_docs=${{ github.event.inputs.max_docs || '' }}" >> "$GITHUB_OUTPUT"
          echo "timeout=${{ github.event.inputs.timeout || '30' }}" >> "$GITHUB_OUTPUT"

  harvest:
    runs-on: ubuntu-latest
    needs: compute-matrix
    strategy:
      fail-fast: false
      matrix:
        shard: ${{ fromJSON(needs.compute-matrix.outputs.shards) }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
      - name: Run shard
        env:
          COVERAGE_TIMEOUT: ${{ needs.compute-matrix.outputs.timeout }}
          COVERAGE_MAX_DOCS: ${{ needs.compute-matrix.outputs.max_docs }}
          SHARD_INDEX: ${{ matrix.shard }}
          SHARD_TOTAL: ${{ needs.compute-matrix.outputs.total }}
        run: |
          python -m scripts.harvest_shard
      - name: Upload shard outputs
        uses: actions/upload-artifact@v4
        with:
          name: shard-${{ matrix.shard }}-of-${{ needs.compute-matrix.outputs.total }}
          path: |
            dataset/document_codes_shard_${{ matrix.shard }}_of_${{ needs.compute-matrix.outputs.total }}.csv
            dataset/document_nocodes_shard_${{ matrix.shard }}_of_${{ needs.compute-matrix.outputs.total }}.csv

  merge:
    runs-on: ubuntu-latest
    needs: harvest
    steps:
      - uses: actions/checkout@v4
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge CSVs
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p dataset

          merge_one() {
            pattern="$1"
            out="$2"
            : > "$out"
            first=1
            while IFS= read -r -d '' f; do
              if [ $first -eq 1 ]; then
                cat "$f" >> "$out"
                first=0
              else
                tail -n +2 "$f" >> "$out"
              fi
            done < <(find artifacts -type f -name "$pattern" -print0 | sort -z)
          }

          merge_one "document_codes_shard_*_of_*.csv" "dataset/document_codes.csv"
          merge_one "document_nocodes_shard_*_of_*.csv" "dataset/document_nocodes.csv"

      - name: Create zip of merged dataset
        id: mkzip
        shell: bash
        run: |
          set -euo pipefail
          ts="$(date -u +%Y%m%d-%H%M%S)"
          (cd dataset && zip -j "dataset-${ts}.zip" document_codes.csv document_nocodes.csv)
          echo "zip_name=dataset/dataset-${ts}.zip" >> "$GITHUB_OUTPUT"

      # === Normalize and compute changes vs latest release (if any) ===
      - name: Download last release normalized file (if exists)
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail
          mkdir -p prev
          gh release download --repo "${{ github.repository }}" \
            --pattern "codes_normalized.csv" \
            --dir prev --clobber || echo "No previous normalized file found; first run?"

      - name: Compute normalized codes and changes
        shell: bash
        run: |
          set -euo pipefail
          python -m scripts.compute_changes \
            dataset/document_codes.csv \
            prev/codes_normalized.csv || \
            python -m scripts.compute_changes dataset/document_codes.csv

      - name: Upload merged & changes artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dataset-merged
          path: |
            dataset/document_codes.csv
            dataset/document_nocodes.csv
            dataset/codes_normalized.csv
            dataset/codes_changes.csv
            ${{ steps.mkzip.outputs.zip_name }}

      # === Create a dated GitHub Release and attach assets ===
      - name: Create GitHub Release and upload assets
        uses: softprops/action-gh-release@v1
        with:
          tag_name: v${{ github.run_id }}-${{ github.run_attempt }}
          name: "Daily dataset â€“ ${{ github.run_id }}"
          body: |
            Automated daily harvest.
            - Shards: ${{ needs.compute-matrix.outputs.total }}
            - Timeout: ${{ needs.compute-matrix.outputs.timeout }}s
          draft: false
          prerelease: false
          make_latest: true
          files: |
            dataset/document_codes.csv
            dataset/document_nocodes.csv
            dataset/codes_normalized.csv
            dataset/codes_changes.csv
            ${{ steps.mkzip.outputs.zip_name }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
